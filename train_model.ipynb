{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b1c04f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    pipeline # Added for testing\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "from functools import partial\n",
    "import time\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA device count: {torch.cuda.device_count()}\")\n",
    "    print(f\"Current CUDA device: {torch.cuda.current_device()}\")\n",
    "    print(f\"Device name: {torch.cuda.get_device_name(torch.cuda.current_device())}\")\n",
    "    device = \"cuda\"\n",
    "else:\n",
    "    print(\"WARNING: CUDA not available. Training will be extremely slow.\")\n",
    "    device = \"cpu\"\n",
    "\n",
    "# --- Configuration You Might Change ---\n",
    "\n",
    "# 1. Model ID from Hugging Face (Python-specific CodeLlama is good for this task)\n",
    "model_id = \"codellama/CodeLlama-7b-Python-hf\"\n",
    "\n",
    "# 2. Path to your training data file (JSON Lines format)\n",
    "#    Make sure this file exists in the 'data' subdirectory relative to the notebook\n",
    "#    or provide the full path.\n",
    "dataset_file = \"data/train.jsonl\"\n",
    "if not os.path.exists(dataset_file):\n",
    "     print(f\"WARNING: Dataset file '{dataset_file}' not found. Please create it or update the path.\")\n",
    "\n",
    "\n",
    "# 3. Directory where the final trained adapter layers will be saved\n",
    "output_dir = \"./requests_codellama_final\"\n",
    "\n",
    "# --- Training Settings ---\n",
    "num_epochs = 10            # How many times the model sees the entire dataset\n",
    "batch_size = 4            # How many examples to process in parallel on the GPU (adjust based on memory)\n",
    "gradient_accumulation_steps = 4 # Process this many batches before updating weights\n",
    "learning_rate = 2e-4       # Model learning rate\n",
    "max_seq_length = 512       # Maximum length of text the model processes at once\n",
    "log_steps = 1              # Print training loss every N steps (1 = very frequent)\n",
    "\n",
    "# --- Quantization & Precision ---\n",
    "use_8bit_quantization = True # Use 8-bit to save memory (recommended for 7B on many GPUs)\n",
    "use_bf16 = torch.cuda.is_available() and torch.cuda.is_bf16_supported() # Use bfloat16 if available\n",
    "\n",
    "print(\"-\" * 30)\n",
    "print(f\"Configuration:\")\n",
    "print(f\"  Model ID: {model_id}\")\n",
    "print(f\"  Dataset: {dataset_file}\")\n",
    "print(f\"  Output Dir: {output_dir}\")\n",
    "print(f\"  Epochs: {num_epochs}\")\n",
    "print(f\"  Batch Size: {batch_size}\")\n",
    "print(f\"  Gradient Accumulation: {gradient_accumulation_steps}\")\n",
    "print(f\"  Effective Batch Size: {batch_size * gradient_accumulation_steps}\")\n",
    "print(f\"  Learning Rate: {learning_rate}\")\n",
    "print(f\"  Max Sequence Length: {max_seq_length}\")\n",
    "print(f\"  Use 8-bit Quantization: {use_8bit_quantization}\")\n",
    "print(f\"  Use BF16 Precision: {use_bf16}\")\n",
    "print(\"-\" * 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1b99ff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Configure Quantization ---\n",
    "if use_8bit_quantization:\n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "        load_in_8bit=True,\n",
    "    )\n",
    "    print(\"Using 8-bit quantization.\")\n",
    "else:\n",
    "    bnb_config = None # No quantization\n",
    "    print(\"Not using 8-bit quantization.\")\n",
    "\n",
    "\n",
    "# --- Determine Data Type ---\n",
    "compute_dtype = torch.bfloat16 if use_bf16 else torch.float16\n",
    "print(f\"Using compute dtype: {compute_dtype}\")\n",
    "\n",
    "# --- Load Model ---\n",
    "print(f\"Loading base model: {model_id}...\")\n",
    "start_time = time.time()\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\", # Handles placing model layers on devices\n",
    "    torch_dtype=compute_dtype, # Load non-quantized parts in this dtype\n",
    ")\n",
    "end_time = time.time()\n",
    "print(f\"Base model loaded in {end_time - start_time:.2f} seconds.\")\n",
    "print(f\"Model device map: {model.hf_device_map}\") # Shows how model is distributed\n",
    "\n",
    "# --- Load Tokenizer ---\n",
    "print(f\"Loading tokenizer for {model_id}...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    print(\"Set tokenizer pad_token to eos_token\")\n",
    "# Ensure padding is done on the right side\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "print(\"Tokenizer loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4a36554",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- Testing Base Model (Before Fine-tuning) ---\")\n",
    "\n",
    "# Sample prompt similar to our fine-tuning task\n",
    "prompt_text = \"How do I send a POST request with JSON data using requests?\"\n",
    "\n",
    "# Format the prompt clearly for the model\n",
    "# Using a common instruction format often helps base models\n",
    "input_text = f\"\"\"Instruction:\n",
    "{prompt_text}\n",
    "\n",
    "Python Code:\n",
    "```python\n",
    "\"\"\"\n",
    "\n",
    "print(f\"Test Prompt:\\n{input_text}\")\n",
    "\n",
    "# Convert prompt text to tokens\n",
    "inputs = tokenizer(input_text, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "# Generate code\n",
    "print(\"\\nGenerating...\")\n",
    "start_time = time.time()\n",
    "with torch.no_grad(): # Disable gradient calculations for inference\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=150,      # Limit output length\n",
    "        temperature=0.6,         # Controls randomness (lower = more predictable)\n",
    "        do_sample=True,          # Use sampling\n",
    "        top_p=0.9,               # Nucleus sampling\n",
    "        pad_token_id=tokenizer.eos_token_id, # Stop when EOS is generated\n",
    "    )\n",
    "end_time = time.time()\n",
    "print(f\"Generation finished in {end_time - start_time:.2f} seconds.\")\n",
    "\n",
    "# Convert output tokens back to text\n",
    "# We skip special tokens like padding or EOS in the decoded output\n",
    "generated_code = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "print(\"\\n--- Base Model Output ---\")\n",
    "# Often the model might just repeat the prompt or generate incomplete/incorrect code initially\n",
    "print(generated_code)\n",
    "print(\"-\" * 25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68d28afb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Description: Load your custom dataset and prepare it in the format needed for training.\n",
    "\n",
    "# --- Define Formatting and Tokenizing Functions ---\n",
    "\n",
    "def create_formatted_input(prompt, completion, tokenizer):\n",
    "    \"\"\" Combines prompt and completion with special tokens for training \"\"\"\n",
    "    # Format: \"Prompt: [Your Prompt]\\nCompletion: [Your Code Snippet]<EOS>\"\n",
    "    return f\"Prompt: {prompt}\\nCompletion: {completion}{tokenizer.eos_token}\"\n",
    "\n",
    "def tokenize_function(examples, tokenizer):\n",
    "    \"\"\" Tokenizes a batch of examples \"\"\"\n",
    "    prompts = examples['prompt']\n",
    "    completions = examples['completion']\n",
    "    # Create the full text for each example in the batch\n",
    "    full_texts = [create_formatted_input(p, c, tokenizer) for p, c in zip(prompts, completions)]\n",
    "    # Tokenize the batch\n",
    "    tokenized_outputs = tokenizer(\n",
    "        full_texts,\n",
    "        max_length=max_seq_length, # Use max_length from config\n",
    "        padding=\"max_length\",      # Pad to max length\n",
    "        truncation=True,           # Truncate longer sequences\n",
    "    )\n",
    "    return tokenized_outputs\n",
    "\n",
    "# --- Load and Process ---\n",
    "print(f\"\\nLoading dataset from: {dataset_file}\")\n",
    "if not os.path.exists(dataset_file):\n",
    "    raise FileNotFoundError(f\"Dataset file not found: {dataset_file}. Please ensure it's in the correct path.\")\n",
    "\n",
    "raw_dataset = load_dataset('json', data_files=dataset_file, split='train')\n",
    "print(f\"Dataset loaded. Number of examples: {len(raw_dataset)}\")\n",
    "print(\"Showing first example:\", raw_dataset[0])\n",
    "\n",
    "print(\"\\nTokenizing dataset (this may take a moment)...\")\n",
    "# Use 'partial' to pass the tokenizer to the mapping function easily\n",
    "tokenize_with_tokenizer = partial(tokenize_function, tokenizer=tokenizer)\n",
    "\n",
    "# Apply the tokenization to the entire dataset\n",
    "# `batched=True` processes multiple examples at once for speed\n",
    "# `remove_columns` cleans up the dataset by removing the original text columns\n",
    "tokenized_dataset = raw_dataset.map(\n",
    "    tokenize_with_tokenizer,\n",
    "    batched=True,\n",
    "    remove_columns=raw_dataset.column_names\n",
    ")\n",
    "print(\"Dataset successfully tokenized.\")\n",
    "print(\"Columns in tokenized dataset:\", tokenized_dataset.column_names) # Should be ['input_ids', 'attention_mask']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4642c65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Description: Set up LoRA to efficiently fine-tune the model by only\n",
    "# training a small number of adapter parameters.\n",
    "\n",
    "print(\"\\n--- Configuring LoRA ---\")\n",
    "\n",
    "# 1. Prepare the quantized model for LoRA training\n",
    "#    (Important step when using quantization + PEFT)\n",
    "model.gradient_checkpointing_enable() # Saves more memory during training\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "print(\"Model prepared for LoRA training.\")\n",
    "\n",
    "# 2. Define LoRA configuration\n",
    "lora_config = LoraConfig(\n",
    "    r=8,                             # LoRA rank (dimension of trainable matrices)\n",
    "    lora_alpha=16,                   # LoRA alpha (scaling factor)\n",
    "    target_modules=[\"q_proj\", \"v_proj\"], # Layers to apply LoRA to (check model architecture if needed)\n",
    "    lora_dropout=0.05,               # Dropout probability for LoRA layers\n",
    "    bias=\"none\",                     # Typically 'none' for LoRA\n",
    "    task_type=\"CAUSAL_LM\"            # Must be set for Causal Language Models\n",
    ")\n",
    "print(\"LoRA configuration created.\")\n",
    "\n",
    "# 3. Apply LoRA to the model\n",
    "model = get_peft_model(model, lora_config)\n",
    "print(\"LoRA applied to the model.\")\n",
    "\n",
    "# 4. Print trainable parameters\n",
    "#    Verify that only a small fraction of parameters are trainable (<1%)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b234585c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Description: Configure the Hugging Face Trainer, which handles the training loop,\n",
    "# optimization, logging, and saving.\n",
    "\n",
    "print(\"\\n--- Setting Up Trainer ---\")\n",
    "\n",
    "# --- Define Training Arguments ---\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=output_dir,               # Directory for checkpoints and final output\n",
    "    num_train_epochs=num_epochs,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "    learning_rate=learning_rate,\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=log_steps,             # Log loss frequently\n",
    "    save_strategy=\"epoch\",               # Save checkpoint every epoch\n",
    "    save_total_limit=1,                  # Keep only the last checkpoint\n",
    "    bf16=use_bf16,                       # Use bfloat16 if supported\n",
    "    fp16=not use_bf16,                   # Use float16 if bfloat16 is not supported\n",
    "    optim=\"paged_adamw_8bit\",            # Optimizer optimized for quantized models\n",
    "    gradient_checkpointing=True,         # Enable gradient checkpointing (memory saving)\n",
    "    report_to=\"tensorboard\",             # Log metrics for TensorBoard visualization\n",
    "    # evaluation_strategy=\"no\",          # No evaluation during training in this simple setup\n",
    "    # load_best_model_at_end=False,      # Not needed without evaluation\n",
    ")\n",
    "print(\"Training Arguments configured.\")\n",
    "\n",
    "# --- Initialize Trainer ---\n",
    "trainer = Trainer(\n",
    "    model=model,                          # Our LoRA-adapted model\n",
    "    args=training_args,                   # Training configuration\n",
    "    train_dataset=tokenized_dataset,      # The tokenized dataset\n",
    "    tokenizer=tokenizer,                  # The tokenizer\n",
    "    data_collator=DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False), # Handles batch padding\n",
    ")\n",
    "print(\"Trainer initialized.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57552518",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Description: Run the training loop. Monitor the 'loss' value in the output -\n",
    "# it should generally decrease, indicating the model is learning.\n",
    "\n",
    "print(\"\\n--- Starting Fine-tuning Training ---\")\n",
    "print(f\"Will train for {num_epochs} epochs.\")\n",
    "print(f\"Loss will be logged every {log_steps} steps.\")\n",
    "print(f\"Checkpoints (and final model) will be saved in '{output_dir}'.\")\n",
    "\n",
    "# Make sure the output directory exists\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Calculate total steps for info\n",
    "total_steps = (len(tokenized_dataset) // (batch_size * gradient_accumulation_steps)) * num_epochs\n",
    "print(f\"Estimated total training steps: {total_steps}\")\n",
    "\n",
    "# Start training!\n",
    "start_time = time.time()\n",
    "train_result = trainer.train() # This runs the training loop\n",
    "end_time = time.time()\n",
    "\n",
    "print(f\"\\n--- Training Finished ---\")\n",
    "print(f\"Training took {end_time - start_time:.2f} seconds.\")\n",
    "\n",
    "# You can optionally display training metrics\n",
    "# print(\"\\nTraining Metrics:\")\n",
    "# print(train_result.metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e2acf3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Description: Save the trained LoRA adapter layers and associated files.\n",
    "# This is the result of your fine-tuning that you'll use for inference.\n",
    "\n",
    "print(\"\\n--- Saving Final Model Adapter ---\")\n",
    "\n",
    "# Ensure the final output directory exists\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Save the LoRA adapter weights, config, tokenizer, etc.\n",
    "# Using trainer.save_model() is convenient as it saves everything needed.\n",
    "trainer.save_model(output_dir)\n",
    "print(f\"Final adapter and tokenizer configuration saved to: {output_dir}\")\n",
    "\n",
    "# You might also want to save the full trainer state if you plan to resume later\n",
    "# trainer.save_state()\n",
    "# print(\"Trainer state saved.\")\n",
    "\n",
    "# Check if the adapter files were actually saved\n",
    "adapter_model_file = os.path.join(output_dir, \"adapter_model.bin\")\n",
    "adapter_config_file = os.path.join(output_dir, \"adapter_config.json\")\n",
    "\n",
    "if os.path.exists(adapter_model_file) and os.path.exists(adapter_config_file):\n",
    "    print(\"Adapter files verified in the output directory.\")\n",
    "else:\n",
    "    print(\"WARNING: Final adapter files not found in the main output directory.\")\n",
    "    print(\"Check if training completed successfully and if there were saving errors.\")\n",
    "    print(\"You might need to load from the last checkpoint directory inside:\", output_dir)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
